nrow(Boston[Boston$rm>8,])
(Boston[Boston$rm>8,])
summary(Boston$crim)
savehistory()
summary(Boston$crim)
library(MASS)
library(ISLR)
install.packages("ISLR")
?Boston
fix(Boston)
fix(Boston)
?fix
names(Boston)
?Boston
lm.fit=lm(med~lstat)
lm.fit=lm(med~lstat,data=Boston)
lm.fit=lm(medv~lstat,data=Boston)
lm.fit
summary(lm.fit)
lm.fit$coefficients
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
?Boston
Boston$lstat[1,1]
Boston$lstat[1]
predict(lm.fit,data.frame(lstat=c(5,10,15))),interval="confidence")
predict(lm.fit,data.frame(lstat=(c(5,10,15))),interval="confidence")
summary(boston?medv)
summary(Boston?medv)
summary(Boston$medv)
summary(Boston$lstat)
install.packages("ISLR")
plot(Boston$medv,Boston$lstat)
?Boston
plot(Boston$lstat,Boston$medv)
abline(lm.fit)
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3,col="red")
plot(Boston$lstat,Boston$medv,col="red")
plot(Boston$lstat,Boston$medv,pch=20)
plot(Boston$lstat,Boston$medv,pch="+")
plot(Boston$lstat,Boston$medv,pch="*")
plot(Boston$lstat,Boston$medv,pch="R")
plot(Boston$lstat,Boston$medv,pch="/")
plot (1:20,1:20,pch=1:20)
par(mfrow=c(2,2))
plot(lm.fit)
plot(predict(lm.fit),residuals(lm.fit))
plot(predict(lm.fit),restudent(lm.fit))
plot(predict(lm.fit),rstudent(lm.fit))
plot(hatvalues(lm.fit))
which.max(plot(hatvalues(lm.fit)))
which.max(hatvalues(lm.fit))
21126909
8663166526
savehistory()
exit
quit
quit
quit()
library(MASS)
quit()
etf = "http://finance.yahoo.com/etf/lists/category/"
etf.table= readHTMLTable(etf)
library(XML)
etf.table= readHTMLTable(etf)
etf.table
str(etf.table)
etf = "finance.yahoo.com/etf/lists/category/"
etf.table= readHTMLTable(etf)
etf = "http://finance.yahoo.com/etf/lists/category/"
etf.table= readHTMLTable(etf)
u = "http://en.wikipedia.org/wiki/List_of_countries_by_population"
tables = readHTMLTable(u)
tables
names(tables)
tables[[2]]
tmp = tables[[2]]
tmp
airline = "http://www.theacsi.org/index.php?option=com_content&view=article&id=147&catid=&Itemid=212&i=Airlines"
airline.table = readHTMLTable(airline, header=T, which=1,stringsAsFactors=F)
View(airline.table)
airline.table = readHTMLTable(airline)
etf.table= readHTMLTable(etf,header=T,which=1,stringsAsFactors=F)
etf
tables = readHTMLTable(u,stringsAsFactors=F)
tables
etf.table= readHTMLTable(etf,stringsAsFactors=F)
etf.table
etf.table= readHTMLTable(etf,which=1,stringsAsFactors=F)
View(etf.table)
etf.table= readHTMLTable(etf,which=2,stringsAsFactors=F)
View(etf.table)
etf.table
install.packages("RCurl")
etf_return = "http://news.morningstar.com/etf/lists/ETFReturns.html?topNum=All&lastRecNum=100000&curField=8&category=0"
etf_return_raw.table = readHTMLTable(etf_return,which=3,stringsAsFactors=F)
funds_return ="http://news.morningstar.com/fund-category-returns"
funds_return_raw.table = readHTMLTable(funds_return,which=1,stringsAsFactors=F)
nr_etf=nrow(etf_return_raw.table) -1
nc_etf=ncol(etf_return_raw.table) -1
library("zoo")
library("fPortfolio")
library("PerformanceAnalytics")
?maxreturnPortfolio
??maxreturnPortfolio
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
source('D:/Analytics/R-code/portfolio_optimization_prediction/CG_CVAR_portman.R')
quit()
quit()
library(qdap)
#Neural Network. With Text
setwd("D:/Analytics/Analytics_edge/competition/data")
set.seed(144)
NewsTrain = read.csv("NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
NewsTest = read.csv("NYTimesBlogTest.csv", stringsAsFactors=FALSE)
NewsTrain$Text = paste(NewsTrain$Headline, NewsTrain$Snippet, sep = ". ")
NewsTest$Text = paste(NewsTest$Headline, NewsTest$Snippet, sep = ". ")
library(tm)
# Then create a corpus from the headline variable. You can use other variables in the dataset for text analytics, but we will just show you how to use this particular variable.
# Note that we are creating a corpus out of the training and testing data.
CorpusHeadline = Corpus(VectorSource(c(NewsTrain$Text, NewsTest$Text)))
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusHeadline)
dtm
inspect(dtm[1000:1005,505:515])
sparse = removeSparseTerms(dtm, 0.99)
sparse
TotWords = as.data.frame(as.matrix(sparse))
# Let's make sure our variable names are okay for R:
colnames(TotWords) = make.names(colnames(TotWords))
#Cluster
distances = dist(TotWords, method = "euclidean")
# Hierarchical clustering
clusterkos = hclust(distances, method = "ward.D")
# Plot the dendrogram
plot(clusterkos)
#6 or 9 clusters will be best
clusterGroups = cutree(clusterkos, k = 9)
table(clusterGroups)
#-------------------------------------------------Normal now--------------------------
CorpusHeadline = Corpus(VectorSource(c(NewsTrain$Headline, NewsTest$Headline)))
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusHeadline)
dtm
inspect(dtm[1000:1005,505:515])
sparse = removeSparseTerms(dtm, 0.99)
sparse
HeadlineWords = as.data.frame(as.matrix(sparse))
# Let's make sure our variable names are okay for R:
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
HeadlineWords$cluster = clusterGroups
# Now we need to split the observations back into the training set and testing set.
# To do this, we can use the head and tail functions in R.
# The head function takes the first "n" rows of HeadlineWords (the first argument to the head function), where "n" is specified by the second argument to the head function.
# So here we are taking the first nrow(NewsTrain) observations from HeadlineWords, and putting them in a new data frame called "HeadlineWordsTrain"
HeadlineWordsTrain = head(HeadlineWords, nrow(NewsTrain))
# The tail function takes the last "n" rows of HeadlineWords (the first argument to the tail function), where "n" is specified by the second argument to the tail function.
# So here we are taking the last nrow(NewsTest) observations from HeadlineWords, and putting them in a new data frame called "HeadlineWordsTest"
HeadlineWordsTest = tail(HeadlineWords, nrow(NewsTest))
# Note that this split of HeadlineWords works to properly put the observations back into the training and testing sets, because of how we combined them together when we first made our corpus.
# Before building models, we want to add back the original variables from our datasets. We'll add back the dependent variable to the training set, and the WordCount variable to both datasets. You might want to add back more variables to use in your model - we'll leave this up to you!
#1 = yes and 0 = no
HeadlineWordsTrain$Popular = ifelse((NewsTrain$Popular == 0), 'No', 'Yes')
HeadlineWordsTrain$Popular = as.factor(HeadlineWordsTrain$Popular)
HeadlineWordsTrain$WordCount = ifelse((log(NewsTrain$WordCount) < 0), 0, log(NewsTrain$WordCount))
HeadlineWordsTest$WordCount = ifelse((log(NewsTest$WordCount) < 0), 0, log(NewsTest$WordCount))
summary(HeadlineWordsTrain)
str(TotWords)
pol<- polarity(NewsTrain$Text)
str(pol)
?sentSplit
HeadlineWordsTrain$polarity = pol[[1]]$polarity
pol<- polarity(NewsTest$Text)
HeadlineWordsTest$polarity = pol[[1]]$polarity
#Random Forest
library(gmodels)
library(ROCR)
library(caret)
library(e1071)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caTools)
install.packages("gmodels")
library(gmodels)
set.seed(21)
numFolds = trainControl( method = "cv", number = 10 )
cpGrid = expand.grid( .mtry = seq(5,6,1))
train(Popular ~ WordCount  + polarity + NewsDesk + SectionName + SubsectionName  +
month + hour + Weekday + WType, data = HeadlineWordsTrain, method = "rf", trControl = numFolds, tuneGrid = cpGrid )
str(HeadlineWordsTrain)
#Neural Network. With Text
setwd("D:/Analytics/Analytics_edge/competition/data")
set.seed(144)
NewsTrain = read.csv("NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
NewsTest = read.csv("NYTimesBlogTest.csv", stringsAsFactors=FALSE)
NewsTrain$Text = paste(NewsTrain$Headline, NewsTrain$Snippet, sep = ". ")
NewsTest$Text = paste(NewsTest$Headline, NewsTest$Snippet, sep = ". ")
library(tm)
# Then create a corpus from the headline variable. You can use other variables in the dataset for text analytics, but we will just show you how to use this particular variable.
# Note that we are creating a corpus out of the training and testing data.
CorpusHeadline = Corpus(VectorSource(c(NewsTrain$Text, NewsTest$Text)))
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusHeadline)
dtm
inspect(dtm[1000:1005,505:515])
sparse = removeSparseTerms(dtm, 0.99)
sparse
TotWords = as.data.frame(as.matrix(sparse))
# Let's make sure our variable names are okay for R:
colnames(TotWords) = make.names(colnames(TotWords))
#Cluster
distances = dist(TotWords, method = "euclidean")
# Hierarchical clustering
clusterkos = hclust(distances, method = "ward.D")
# Plot the dendrogram
plot(clusterkos)
#6 or 9 clusters will be best
clusterGroups = cutree(clusterkos, k = 9)
table(clusterGroups)
#-------------------------------------------------Normal now--------------------------
CorpusHeadline = Corpus(VectorSource(c(NewsTrain$Headline, NewsTest$Headline)))
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusHeadline)
dtm
inspect(dtm[1000:1005,505:515])
sparse = removeSparseTerms(dtm, 0.99)
sparse
HeadlineWords = as.data.frame(as.matrix(sparse))
# Let's make sure our variable names are okay for R:
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
HeadlineWords$cluster = clusterGroups
# Now we need to split the observations back into the training set and testing set.
# To do this, we can use the head and tail functions in R.
# The head function takes the first "n" rows of HeadlineWords (the first argument to the head function), where "n" is specified by the second argument to the head function.
# So here we are taking the first nrow(NewsTrain) observations from HeadlineWords, and putting them in a new data frame called "HeadlineWordsTrain"
HeadlineWordsTrain = head(HeadlineWords, nrow(NewsTrain))
# The tail function takes the last "n" rows of HeadlineWords (the first argument to the tail function), where "n" is specified by the second argument to the tail function.
# So here we are taking the last nrow(NewsTest) observations from HeadlineWords, and putting them in a new data frame called "HeadlineWordsTest"
HeadlineWordsTest = tail(HeadlineWords, nrow(NewsTest))
# Note that this split of HeadlineWords works to properly put the observations back into the training and testing sets, because of how we combined them together when we first made our corpus.
# Before building models, we want to add back the original variables from our datasets. We'll add back the dependent variable to the training set, and the WordCount variable to both datasets. You might want to add back more variables to use in your model - we'll leave this up to you!
#1 = yes and 0 = no
HeadlineWordsTrain$Popular = ifelse((NewsTrain$Popular == 0), 'No', 'Yes')
HeadlineWordsTrain$Popular = as.factor(HeadlineWordsTrain$Popular)
HeadlineWordsTrain$WordCount = ifelse((log(NewsTrain$WordCount) < 0), 0, log(NewsTrain$WordCount))
HeadlineWordsTest$WordCount = ifelse((log(NewsTest$WordCount) < 0), 0, log(NewsTest$WordCount))
summary(HeadlineWordsTrain)
library(qdap)
pol<- polarity(NewsTrain$Text)
HeadlineWordsTrain$polarity = pol[[1]]$polarity
pol<- polarity(NewsTest$Text)
HeadlineWordsTest$polarity = pol[[1]]$polarity
#Lets get some date and time :)
HeadlineWordsTrain$PubDate = strptime(NewsTrain$PubDate, "%Y-%m-%d %H:%M:%S")
HeadlineWordsTest$PubDate = strptime(NewsTest$PubDate, "%Y-%m-%d %H:%M:%S")
HeadlineWordsTrain$Weekday = HeadlineWordsTrain$PubDate$wday
HeadlineWordsTest$Weekday = HeadlineWordsTest$PubDate$wday
HeadlineWordsTrain$month = HeadlineWordsTrain$PubDate$mon
HeadlineWordsTest$month = HeadlineWordsTest$PubDate$mon
HeadlineWordsTrain$mday = HeadlineWordsTrain$PubDate$mday
HeadlineWordsTest$mday = HeadlineWordsTest$PubDate$mday
HeadlineWordsTrain$hour = HeadlineWordsTrain$PubDate$hour
HeadlineWordsTest$hour = HeadlineWordsTest$PubDate$hour
HeadlineWordsTrain$WType = ifelse((HeadlineWordsTrain$Weekday == 0) | (HeadlineWordsTrain$Weekday == 6), 1, 0)
HeadlineWordsTest$WType = ifelse((HeadlineWordsTest$Weekday == 0) | (HeadlineWordsTest$Weekday == 6), 1, 0)
#Other Vars
HeadlineWordsTrain$NewsDesk = ifelse((NewsTrain$NewsDesk != ''), NewsTrain$NewsDesk, "Other")
HeadlineWordsTest$NewsDesk = ifelse((NewsTest$NewsDesk != ''), NewsTest$NewsDesk, "Other")
HeadlineWordsTrain$NewsDesk = as.factor(HeadlineWordsTrain$NewsDesk)
HeadlineWordsTest$NewsDesk <- factor(HeadlineWordsTest$NewsDesk, levels = levels(HeadlineWordsTrain$NewsDesk))
HeadlineWordsTrain$SectionName = ifelse((NewsTrain$SectionName != ''), NewsTrain$SectionName, "Other")
HeadlineWordsTest$SectionName = ifelse((NewsTest$SectionName != ''), NewsTest$SectionName, "Other")
HeadlineWordsTrain$SectionName = as.factor(HeadlineWordsTrain$SectionName)
HeadlineWordsTest$SectionName <- factor(HeadlineWordsTest$SectionName, levels = levels(HeadlineWordsTrain$SectionName))
HeadlineWordsTrain$SubsectionName = ifelse((NewsTrain$SubsectionName != ''), NewsTrain$SubsectionName, "Other")
HeadlineWordsTest$SubsectionName = ifelse((NewsTest$SubsectionName != ''), NewsTest$SubsectionName, "Other")
HeadlineWordsTrain$SubsectionName = as.factor(HeadlineWordsTrain$SubsectionName)
HeadlineWordsTest$SubsectionName <- factor(HeadlineWordsTest$SubsectionName, levels = levels(HeadlineWordsTrain$SubsectionName))
#Random Forest
library(gmodels)
library(ROCR)
library(caret)
library(e1071)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caTools)
str(HeadlineWordsTrain)
str(HeadlineWordsTrain$cluster)
str(HeadlineWords$cluster)
clusterGroups
rm(list=ls())
#Neural Network. With Text
setwd("D:/Analytics/Analytics_edge/competition/data")
set.seed(144)
NewsTrain = read.csv("NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
NewsTest = read.csv("NYTimesBlogTest.csv", stringsAsFactors=FALSE)
NewsTrain$Text = paste(NewsTrain$Headline, NewsTrain$Snippet, sep = ". ")
NewsTest$Text = paste(NewsTest$Headline, NewsTest$Snippet, sep = ". ")
library(tm)
# Then create a corpus from the headline variable. You can use other variables in the dataset for text analytics, but we will just show you how to use this particular variable.
# Note that we are creating a corpus out of the training and testing data.
CorpusHeadline = Corpus(VectorSource(c(NewsTrain$Text, NewsTest$Text)))
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusHeadline)
dtm
inspect(dtm[1000:1005,505:515])
sparse = removeSparseTerms(dtm, 0.99)
sparse
TotWords = as.data.frame(as.matrix(sparse))
# Let's make sure our variable names are okay for R:
colnames(TotWords) = make.names(colnames(TotWords))
#Cluster
distances = dist(TotWords, method = "euclidean")
# Hierarchical clustering
clusterkos = hclust(distances, method = "ward.D")
# Plot the dendrogram
plot(clusterkos)
#6 or 9 clusters will be best
clusterGroups = cutree(clusterkos, k = 9)
table(clusterGroups)
#-------------------------------------------------Normal now--------------------------
CorpusHeadline = Corpus(VectorSource(c(NewsTrain$Headline, NewsTest$Headline)))
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusHeadline)
dtm
inspect(dtm[1000:1005,505:515])
sparse = removeSparseTerms(dtm, 0.99)
sparse
HeadlineWords = as.data.frame(as.matrix(sparse))
# Let's make sure our variable names are okay for R:
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
HeadlineWords$cluster = clusterGroups
# Now we need to split the observations back into the training set and testing set.
# To do this, we can use the head and tail functions in R.
# The head function takes the first "n" rows of HeadlineWords (the first argument to the head function), where "n" is specified by the second argument to the head function.
# So here we are taking the first nrow(NewsTrain) observations from HeadlineWords, and putting them in a new data frame called "HeadlineWordsTrain"
HeadlineWordsTrain = head(HeadlineWords, nrow(NewsTrain))
# The tail function takes the last "n" rows of HeadlineWords (the first argument to the tail function), where "n" is specified by the second argument to the tail function.
# So here we are taking the last nrow(NewsTest) observations from HeadlineWords, and putting them in a new data frame called "HeadlineWordsTest"
HeadlineWordsTest = tail(HeadlineWords, nrow(NewsTest))
# Note that this split of HeadlineWords works to properly put the observations back into the training and testing sets, because of how we combined them together when we first made our corpus.
# Before building models, we want to add back the original variables from our datasets. We'll add back the dependent variable to the training set, and the WordCount variable to both datasets. You might want to add back more variables to use in your model - we'll leave this up to you!
#1 = yes and 0 = no
HeadlineWordsTrain$Popular = ifelse((NewsTrain$Popular == 0), 'No', 'Yes')
HeadlineWordsTrain$Popular = as.factor(HeadlineWordsTrain$Popular)
HeadlineWordsTrain$WordCount = ifelse((log(NewsTrain$WordCount) < 0), 0, log(NewsTrain$WordCount))
HeadlineWordsTest$WordCount = ifelse((log(NewsTest$WordCount) < 0), 0, log(NewsTest$WordCount))
summary(HeadlineWordsTrain)
library(qdap)
pol<- polarity(NewsTrain$Text)
HeadlineWordsTrain$polarity = pol[[1]]$polarity
pol<- polarity(NewsTest$Text)
HeadlineWordsTest$polarity = pol[[1]]$polarity
#Lets get some date and time :)
HeadlineWordsTrain$PubDate = strptime(NewsTrain$PubDate, "%Y-%m-%d %H:%M:%S")
HeadlineWordsTest$PubDate = strptime(NewsTest$PubDate, "%Y-%m-%d %H:%M:%S")
HeadlineWordsTrain$Weekday = HeadlineWordsTrain$PubDate$wday
HeadlineWordsTest$Weekday = HeadlineWordsTest$PubDate$wday
HeadlineWordsTrain$month = HeadlineWordsTrain$PubDate$mon
HeadlineWordsTest$month = HeadlineWordsTest$PubDate$mon
HeadlineWordsTrain$mday = HeadlineWordsTrain$PubDate$mday
HeadlineWordsTest$mday = HeadlineWordsTest$PubDate$mday
HeadlineWordsTrain$hour = HeadlineWordsTrain$PubDate$hour
HeadlineWordsTest$hour = HeadlineWordsTest$PubDate$hour
HeadlineWordsTrain$WType = ifelse((HeadlineWordsTrain$Weekday == 0) | (HeadlineWordsTrain$Weekday == 6), 1, 0)
HeadlineWordsTest$WType = ifelse((HeadlineWordsTest$Weekday == 0) | (HeadlineWordsTest$Weekday == 6), 1, 0)
#Other Vars
HeadlineWordsTrain$NewsDesk = ifelse((NewsTrain$NewsDesk != ''), NewsTrain$NewsDesk, "Other")
HeadlineWordsTest$NewsDesk = ifelse((NewsTest$NewsDesk != ''), NewsTest$NewsDesk, "Other")
HeadlineWordsTrain$NewsDesk = as.factor(HeadlineWordsTrain$NewsDesk)
HeadlineWordsTest$NewsDesk <- factor(HeadlineWordsTest$NewsDesk, levels = levels(HeadlineWordsTrain$NewsDesk))
HeadlineWordsTrain$SectionName = ifelse((NewsTrain$SectionName != ''), NewsTrain$SectionName, "Other")
HeadlineWordsTest$SectionName = ifelse((NewsTest$SectionName != ''), NewsTest$SectionName, "Other")
HeadlineWordsTrain$SectionName = as.factor(HeadlineWordsTrain$SectionName)
HeadlineWordsTest$SectionName <- factor(HeadlineWordsTest$SectionName, levels = levels(HeadlineWordsTrain$SectionName))
HeadlineWordsTrain$SubsectionName = ifelse((NewsTrain$SubsectionName != ''), NewsTrain$SubsectionName, "Other")
HeadlineWordsTest$SubsectionName = ifelse((NewsTest$SubsectionName != ''), NewsTest$SubsectionName, "Other")
HeadlineWordsTrain$SubsectionName = as.factor(HeadlineWordsTrain$SubsectionName)
HeadlineWordsTest$SubsectionName <- factor(HeadlineWordsTest$SubsectionName, levels = levels(HeadlineWordsTrain$SubsectionName))
#Random Forest
library(gmodels)
library(ROCR)
library(caret)
library(e1071)
library(rpart)
library(rpart.plot)
library(randomForest)
library(caTools)
str(HeadlineWordsTrain)
rm(list=ls())
rm(list=ls())
setwd("D:/Analytics/Analytics_edge/competition/data")
set.seed(144)
NewsTrain = read.csv("NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
NewsTest = read.csv("NYTimesBlogTest.csv", stringsAsFactors=FALSE)
NewsTrain$Text = paste(NewsTrain$Headline, NewsTrain$Snippet, sep = ". ")
NewsTest$Text = paste(NewsTest$Headline, NewsTest$Snippet, sep = ". ")
library(tm)
# Then create a corpus from the headline variable. You can use other variables in the dataset for text analytics, but we will just show you how to use this particular variable.
# Note that we are creating a corpus out of the training and testing data.
CorpusHeadline = Corpus(VectorSource(c(NewsTrain$Text, NewsTest$Text)))
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusHeadline)
dtm
inspect(dtm[1000:1005,505:515])
sparse = removeSparseTerms(dtm, 0.99)
sparse
TotWords = as.data.frame(as.matrix(sparse))
# Let's make sure our variable names are okay for R:
colnames(TotWords) = make.names(colnames(TotWords))
#Cluster
distances = dist(TotWords, method = "euclidean")
rm(list=ls())
setwd("D:/Analytics/Analytics_edge/competition/data")
set.seed(144)
NewsTrain = read.csv("NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
NewsTest = read.csv("NYTimesBlogTest.csv", stringsAsFactors=FALSE)
NewsTrain$Text = paste(NewsTrain$Headline, NewsTrain$Snippet, sep = ". ")
NewsTest$Text = paste(NewsTest$Headline, NewsTest$Snippet, sep = ". ")
library(tm)
# Then create a corpus from the headline variable. You can use other variables in the dataset for text analytics, but we will just show you how to use this particular variable.
# Note that we are creating a corpus out of the training and testing data.
CorpusHeadline = Corpus(VectorSource(c(NewsTrain$Text, NewsTest$Text)))
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusHeadline)
dtm
inspect(dtm[1000:1005,505:515])
sparse = removeSparseTerms(dtm, 0.99)
sparse
TotWords = as.data.frame(as.matrix(sparse))
# Let's make sure our variable names are okay for R:
colnames(TotWords) = make.names(colnames(TotWords))
#Cluster
distances = dist(TotWords, method = "euclidean")
rm(CorpusHeadline)
distances = dist(TotWords, method = "euclidean")
rm(sparse)
rm(dtm)
distances = dist(TotWords, method = "euclidean")
distances = dist(TotWords, method = "euclidean")
?Boruta
??Boruta
quit()
